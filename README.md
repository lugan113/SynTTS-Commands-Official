# Syntts-Commands-Officialï¼šOn-Device KWS via Synthetic Speech

<!-- Badges -->
<div align="center">
  
  [![arXiv](https://img.shields.io/badge/arXiv-2511.07821-b31b1b.svg)](https://arxiv.org/abs/2511.07821)
  [![Dataset](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Dataset-ffd21e)](https://huggingface.co/datasets/lugan/SynTTS-Commands-Media-Dataset)
  [![Benchmarks](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Benchmarks-ffd21e)](https://huggingface.co/datasets/lugan/SynTTS-Commands-Media-Benchmarks)
  [![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

</div>

<br>

<p align="center">
  <strong>Official Implementation of "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech"</strong>
</p>

<p align="center">
  <a href="#-introduction">Introduction</a> â€¢
  <a href="#-resources">Resources</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-benchmark-results">Benchmarks</a> â€¢
  <a href="#-citation">Citation</a>
</p>

---

## ðŸ“– Introduction

**SynTTS-Commands** is a large-scale, multilingual (English & Chinese) synthetic speech command dataset designed for **low-power Keyword Spotting (KWS)** tasks. Generated using state-of-the-art TTS technology (CosyVoice 2), it addresses the data scarcity bottleneck in TinyML and Edge AI.

## ðŸ”— Resources

| Resource | Description | Link |
| :--- | :--- | :--- |
| **ðŸ“„ Paper** | Full technical report and analysis | [arXiv:2511.07821](https://arxiv.org/abs/2511.07821) |
| **ðŸ’¾ Dataset** | **384k+** Audio samples (Wave files) | [ðŸ¤— HF Dataset](https://huggingface.co/datasets/lugan/SynTTS-Commands-Media-Dataset) |
| **ðŸ§  Models** | Pre-trained checkpoints for benchmarks | [ðŸ¤— HF Models](https://huggingface.co/datasets/lugan/SynTTS-Commands-Media-Benchmarks) |


## ðŸŽ¯ Media Command Categories

### English Media Control Commands (23 Classes)

Playback Control: "Play", "Pause", "Resume", "Play from start", "Repeat song"

Navigation: "Previous track", "Next track", "Last song", "Skip song", "Jump to first track"

Volume Control: "Volume up", "Volume down", "Mute", "Set volume to 50%", "Max volume"

Communication: "Answer call", "Hang up", "Decline call"

Wake Words: "Hey Siri", "OK Google", "Hey Google", "Alexa", "Hi Bixby"

### Chinese Media Control Commands (25 Classes)

Playback Control: "æ’­æ”¾", "æš‚åœ", "ç»§ç»­æ’­æ”¾", "ä»Žå¤´æ’­æ”¾", "å•æ›²å¾ªçŽ¯"

Navigation: "ä¸Šä¸€é¦–", "ä¸‹ä¸€é¦–", "ä¸Šä¸€æ›²", "ä¸‹ä¸€æ›²", "è·³åˆ°ç¬¬ä¸€é¦–", "æ’­æ”¾ä¸Šä¸€å¼ ä¸“è¾‘"

Volume Control: "å¢žå¤§éŸ³é‡", "å‡å°éŸ³é‡", "é™éŸ³", "éŸ³é‡è°ƒåˆ°50%", "éŸ³é‡æœ€å¤§"

Communication: "æŽ¥å¬ç”µè¯", "æŒ‚æ–­ç”µè¯", "æ‹’æŽ¥æ¥ç”µ"

Wake Words: "å°çˆ±åŒå­¦", "Hello å°æ™º", "å°è‰ºå°è‰º", "å—¨ ä¸‰æ˜Ÿå°è´", "å°åº¦å°åº¦", "å¤©çŒ«ç²¾çµ"


## ðŸ“ˆ Benchmark Results and Analysis

We present a comprehensive benchmark of **six representative acoustic models** on the SynTTS-Commands-Media Dataset across both English (EN) and Chinese (ZH) subsets. All models are evaluated in terms of **classification accuracy**, **cross-entropy loss**, and **parameter count**, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.

### Performance Summary

| Model | EN Loss | EN Accuracy | EN Params | ZH Loss | ZH Accuracy | ZH Params |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **MicroCNN** | 0.2304 | 93.22% | 4,189 | 0.5579 | 80.14% | 4,255 |
| **DS-CNN** | 0.0166 | 99.46% | 30,103 | 0.0677 | 97.18% | 30,361 |
| **TC-ResNet** | 0.0347 | 98.87% | 68,431 | 0.0884 | 96.56% | 68,561 |
| **CRNN** | **0.0163** | **99.50%** | 1.08M | 0.0636 | **97.42%** | 1.08M |
| **MobileNet-V1** | 0.0167 | **99.50%** | 2.65M | **0.0552** | 97.92% | 2.65M |
| **EfficientNet** | 0.0182 | 99.41% | 4.72M | 0.0701 | 97.93% | 4.72M |

### ðŸ” Key Findings

Our results demonstrate that the **SynTTS-Commands-Media** dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over **99.4% accuracy on English** and nearly **98% on Chinese**, confirming the datasetâ€™s quality and suitability for real-world deployment.

- **Top Performers**: Among all models, **CRNN** attains the best English accuracy (**99.50%**) and the lowest loss (0.0163). **MobileNet-V1** yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNNâ€™s 99.50% accuracy). Interestingly, **EfficientNet** shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1, suggesting better calibration or robustness despite a higher loss.

- **Accuracy-Complexity Trade-off**: Lightweight models exhibit a clear trade-off. **MicroCNN**, with only ~4.2K parameters, achieves 93.22% accuracy on English but drops to 80.14% on Chinese, highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (>96.5% in both languages), underscoring their efficiency for resource-constrained applications.

Overall, the benchmark establishes strong baselines across a wide spectrum of model scalesâ€”from ultra-light MicroCNN to modern EfficientNetâ€”demonstrating that moderate-complexity models can deliver near-SOTA performance suitable for edge deployment.

## ðŸ“œ Citation

If you use these **pre-trained models** or the **SynTTS-Commands dataset** in your research, please cite our paper:

**[SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech](https://arxiv.org/abs/2511.07821)**

```bibtex
@misc{gan2025synttscommands,
      title={SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech}, 
      author={Lu Gan and Xi Li},
      year={2025},
      eprint={2511.07821},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2511.07821}, 
      doi={10.48550/arXiv.2511.07821}
}


